\documentclass[letterpaper,12pt]{article}
\setlength{\headheight}{14.49998pt}
\usepackage{a4wide}
\usepackage{fancyhdr}
\usepackage{lipsum,graphicx}
\usepackage{amsmath, amsfonts, amssymb, ragged2e}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{times}
\graphicspath{ {./Project/} }
\title{CPEN 355 Final project}
\author{Tom Wang}
\date{Fall, 2023}

\fancypagestyle{plain}{
    \fancyhf{}
    \fancyhead[L]{Tom Wang}
    \fancyhead[C]{76340348}
    \fancyhead[R]{\thepage}
}

\begin{document}
\maketitle
\thispagestyle{plain}


This is the final project for CPEN 355. Specific code implementation should be found in the Jupyter notebook code file attached in the same zip file.
\section{Introdution and Background}
Compared to images or videos, songs are much easier to analyze using computers since they carry much less information. And it is probably one of the first massive uses of AI\-generated content: Sony released a song \href{https://www.youtube.com/watch?v=LSHZ_b05W7o}{Daddy's Car} in 2016 which is a song generated by training on The Bettles.

Motivated by a personal interest in music, I have chosen to undertake this machine-learning project. My primary aim is to leverage machine learning techniques to analyze and gain insights from some of the most listened-to songs. The multifaceted objectives of this project include:

\begin{enumerate}
    \item Classifying songs based on various criteria such as singer, genre, and other relevant attributes. (Classifications learned in class)
    \item Developing a predictive model to assess whether a given song is likely to be enjoyed or not based on certain features. (Maybe I can understand how the players recommend songs)
    \item Further exploring and quantifying similarities between songs, providing a nuanced understanding of musical relationships. (Plagiarism in composition is a hot topic there)
    \item Also, I can identify some signatures of the songs by comparing different parts of songs.
\end{enumerate}

As for the data, I decided to run the learning on my local computer since music files are large and it takes time to upload. But I do not have a good PC, so I collected 16 most listened-to signers in my player, which is about 220 songs.

\section{Preparing}
Below is my implementation summary, which should be in the same order as my code. Detailed implementation explanation should be found in the actual code which is in the same file.
\subsection{Preprocess data}
\subsubsection{Thought process}
Although I chose the least quality data, the sample rate of the MP3 files is 22050. So 22050 samples per second which is a lot.

At the same time, human ears can hear from 20Hz to 20000Hz approximately. So to analyze a sound wave I need to apply about 20000 times of FFT per analysis.

However, if I only apply FFT for the whole wave, then it is just a measure of the total proportion of each frequency in the wave which does not give much information.

So I need to cut the audio file into very small chunks and analyze each chunk separately. If I choose the chunk to be 0.08s and songs are on average 4s long. I will get:
\[\frac{4}{0.08}\times 22050 = 1102500\]
data cells with only two attributes: time and frequency.

To save my PC, I decided to take three snippets of 10s from start, mid and end to analyze. At the same time, I just analyze the \href{https://pages.mtu.edu/~suits/notefreqs.html}{music node frequency} from $A_1$ to $C_7$ which should be the notes used in most songs.

\subsubsection{Processing algorithm}
\begin{enumerate}
    \item Read the song from my drive
    \item Cut up 10s chunks from the start mid and end.
    \item Apply FFT to each chunk for every 0.08s snippets
    \item Collect the data and write them in separate $.csv$ files so that I do not need to preprocess all the time.
\end{enumerate}

\subsubsection{Data spliting}
Just happened to be that there were a few songs that I downloaded from the player which is not properly labeled in the MP3 file. The following code would get `None' from the file. So naturally, these would become my test data sets and wait for the rest to train the model.
\begin{lstlisting}
    def get_mp3_info(mp3_filename):
    # Initialize Eyed3
    audiofile = eyed3.load(mp3_filename)

    # Get artist and genre from the MP3 file's metadata
    artist = audiofile.tag.artist
    #genre = audiofile.tag.genre.name
    title = audiofile.tag.title

    return artist, title
\end{lstlisting}
So I have 220 songs in total, 10 of them are test data.
\subsubsection{Reduce the dimension}
With the current size of the snippets, I have $125\times 64$ data points for each snippet. So I need to reduce the dimension.

To Figure out the best way to reduce the dimension, I tried the following methods:\begin{enumerate}
    \item Grab a sample snippet
    \item try PCA with different number of components\begin{lstlisting}
#Check the cumulative variance for each number of components
for k in range(5, 60, 5):
    pca = PCA(n_components=k)
    reduced_data = pca.fit_transform(df)
    cumulative_var = np.sum(pca.explained_variance_ratio_[:k + 1])
    print(k, cumulative_var)
        \end{lstlisting}
    \item Compare the cumulative variance with the number of components and choose the one that gets most of the variance the decent number of components.
\end{enumerate}
Here is the result: \begin{lstlisting}
        5 0.7254756251686398
        10 0.8747829463301018
        15 0.93963397065257
        20 0.9707914244773223
        25 0.9860049325538303
        30 0.9927169174060696
        35 0.9964934674252495
        40 0.9986133518375307
        45 0.999622615506004
        50 0.9999261757883421
        55 1.0000000000000002
\end{lstlisting}
Looking at the result, I decided to use 10 components. More than 85\% of the variance is explained by 10 components, which is good enough for me. So now each snippet is reduced to $64\times 10$ data points.
\subsection{train the model}
\subsubsection{Choosing models}
I decided to use SVM and NN to train the model. Both of them are good at the classification of high-dimensional data for this specific problem. I did not choose decision tree because it is not good at high dimensional data.

I used SVM out of sklearn. Then I used Tensorflow to build the NN. (I tried Pytorch but I can not install on my PC for some reason. But TensorFlow looks similar in the training process and syntax. So I just used Tensorflow.)

Parameters for each model are chosen by grid search.\begin{itemize}
    \item For SVM, it comes with a built-in grid search function. So I just need to specify the range of parameters and it will do the rest.\begin{lstlisting}[language=Python]
        param_grid = {
            'C': [0.1, 1, 5, 10, 100],
            'kernel': ['linear', 'rbf', 'poly'],
            #'gamma': [0.1, 1, 10, 'auto']
        }
    \end{lstlisting}
          Note that I commented out the gamma parameter since it caused a fixation to predict everything to be the same since there is a dominant class in the data set. (But I do not think it was that bad like only 20/220 songs are from the same singer)

          All parameters are chosen by the grid search function dynamically. And the best parameters would be printed out after each training.

    \item For NN, it seems that there is no built-in grid search function, so I had to do it manually. But here are the parameters that I explored: \begin{lstlisting}[language=Python]
        param_grid = {
            'units': [16, 32, 64, 128, 256],
            'epochs': [5, 10, 20, 30],
            'batch_size': [16, 32, 64]
        }
    \end{lstlisting}
          And I basically tried all the combinations of the parameters, using three for loops\begin{lstlisting}[language=Python]
for units in param_grid['units']:
for epochs in param_grid['epochs']:
for batch_size in param_grid['batch_size']:
    # Create a model
    model = tf.keras.models.Sequential([
        tf.keras.layers.Flatten(input_shape=(X.shape[1],)),
        tf.keras.layers.Dense(units, activation='relu'),
        # line below could limit the output to be 16
        tf.keras.layers.Dense(16, 
        activation=tf.keras.activations.softmax, name='output')
        # Not sure how exactly it works, but it works.
    ])
    model.compile(optimizer='adam',
        loss=tf.keras.losses
        .SparseCategoricalCrossentropy(from_logits=True),
        metrics=['accuracy'])
    
    # Train the model
    model.fit(X, y_tensor, epochs=epochs, batch_size=batch_size)

            ......
    \end{lstlisting}
          Here are a few things I noticed when training with NN:\begin{itemize}
              \item It does not take String as the label, so I need to convert the label to Integer. (Maybe it does, I did not find a way to do it)
              \item Then its predictions would be out of bounds. I only have 16 labels but sometimes it predicts 49 or something wild.
              \item After some researching, I put the last line there in the model instantiation. It seems like it limits the output to be 16, which is the number of singers. Not sure how exactly it works, but it works.
          \end{itemize}
\end{itemize}
\section{Training and result}
Training is pretty straightforward. I just need to call the train function for each model. And the best parameters would be printed out after each training. Here is the iterative procedure:\begin{enumerate}
    \item Read the data from the $.csv$ file (start, mid, end)
    \item Train the model both with SVM and NN
    \item Print out the best parameters with the accuracy on the testing data set
\end{enumerate}
\subsection{Result}

\subsubsection{Fixation try with gamma in SVM}
Before I decide not to use gamma, I got the following result:\begin{lstlisting}
    +-----------+-------------------------------------------+
    | Model     | Parameters                                |
    +===========+===========================================+
    | Start SVM | {'C': 0.1, 'gamma': 0.1, 'kernel': 'rbf'} |
    +-----------+-------------------------------------------+
    | Start NN  | (16, 20, 16)                              |
    +-----------+-------------------------------------------+
    | Mid SVM   | {'C': 0.1, 'gamma': 0.1, 'kernel': 'rbf'} |
    +-----------+-------------------------------------------+
    | Mid NN    | (128, 5, 16)                              |
    +-----------+-------------------------------------------+
    | End SVM   | {'C': 0.1, 'gamma': 0.1, 'kernel': 'rbf'} |
    +-----------+-------------------------------------------+
    | End NN    | (16, 10, 16)                              |
    +-----------+-------------------------------------------+
\end{lstlisting}
And here is the prediction results:

\includegraphics*[scale = 0.8]{./SVM fixation bar.png}

The SVM model is not very good at predicting the test data. It tends to predict everything to be the same. With this specific parameter setting, it would predict everything to be the same singer and it happens that the testing data set contains 3 songs from the same singer. So it predicts everything to be the same singer. And it performs very well on the training data set.

\subsubsection{Try without gamma in SVM}
Then I decided to remove that gamma and train again\begin{lstlisting}
    +-----------+---------------------------+
    | Model     | Parameters                |
    +===========+===========================+
    | Start SVM | {'C': 1, 'kernel': 'rbf'} |
    +-----------+---------------------------+
    | Start NN  | (256, 10, 16)             |
    +-----------+---------------------------+
    | Mid SVM   | {'C': 1, 'kernel': 'rbf'} |
    +-----------+---------------------------+
    | Mid NN    | (16, 5, 32)               |
    +-----------+---------------------------+
    | End SVM   | {'C': 1, 'kernel': 'rbf'} |
    +-----------+---------------------------+
    | End NN    | (256, 5, 32)              |
    +-----------+---------------------------+
    \end{lstlisting}
\includegraphics*[scale = 0.8]{./SVM fixed bar.png}

Note that the performance on SVM did not change at all, but SVM actually predicts all kinds of singers. So it is not a fixation problem. It is just not good at predicting the test data set.

Also, the NN model performance changed a lot. But I did not do any optimization between the two trainings. So I think it is just a random thing.

\subsubsection{Try with all snippets}
The performance did not match my expectations at all, so I suspect that the snippets are too short. So I decided to try with all the snippets. So for one time, I train with all start, mid and end snippets. I twisted a bit with my PCA algorithm to make it work with all snippets.

Now here is the reuslt:\begin{lstlisting}[language=Python]
    +-----------+---------------------------+
    | Model     | Parameters                |
    +===========+===========================+
    | Start SVM | {'C': 1, 'kernel': 'rbf'} |
    +-----------+---------------------------+
    | Start NN  | (32, 30, 32)              |
    +-----------+---------------------------+
    | Mid SVM   | {'C': 1, 'kernel': 'rbf'} |
    +-----------+---------------------------+
    | Mid NN    | (256, 30, 32)             |
    +-----------+---------------------------+
    | End SVM   | {'C': 1, 'kernel': 'rbf'} |
    +-----------+---------------------------+
    | End NN    | (128, 20, 64)             |
    +-----------+---------------------------+
    | All SVM   | {'C': 1, 'kernel': 'rbf'} |
    +-----------+---------------------------+
    | All NN    | (16, 30, 32)              |
    +-----------+---------------------------+
\end{lstlisting}
\includegraphics*[scale=0.8]{All data.png}

I used abbreviations here so that they can fit in the same graph.

So SVM performance is still very constant. But NN is still very random. I rerun the all model training multiple times and the result was very random. So I think it is just a random thing.

\section{Discussion and conclusion}
\begin{itemize}
    \item My primary guess of the training is that NN performs better than SVM given that it is 20 times the training time, which is the case.
    \item However, I was also expecting that the mid-snippets should have a better performance than the start and end snippets since it is the most representative part of the song (most likely that the singer is actually singing in that part). However, the result did not match my expectations.
    \item Most importantly, I found that NN has a very unstable performance. I rerun the training multiple times and the result is very random. I think it is because of the random initialization of the weights. But I am not sure how to fix it.
    \item In contrast, SVM is very stable. It always predicts the same thing with the same model (the same parameters). But, it is not good at predicting the test data set.
\end{itemize}
\subsection{Future improvement}
\begin{itemize}
    \item I think the most important thing is to find a way to make NN more stable. I think it is because of the random initialization of the weights. But I am not sure how to fix it.
    \item Also, I think I should try to use the whole song as the snippet. I think it would be more representative. But it would take a lot of time to train. Maybe when I get a better PC.
    \item Having a cross-validation would be nice.
    \item Testing data size could increase as well. This way, it might avoid the fixation problem in SVM so that fixation would not reach the best accuracy.\begin{itemize}
        \item I think I can use the whole song as the testing data set. This way, it would be more representative.
        \item Then I should probably try to use all the frequency ranges that could be heard by human ears. The FFT work I have done now only covers the music node frequency. But I think the whole frequency range would be more representative. Tone color and overtones are also important features of a song (Both to the singer and the instruments). But these frequencies do not live in the music node frequency range.
        \item Number of songs could increase as well. SVM complains about the dominant class in the data set. Also, it complains that some classes have few samples so it can not split the data set into validation and training sets. So I think I should increase the number of songs.
        \item All these probably would take a lot of time to train. So I should probably only try if I have a better PC (I increased from 64*10 to 64*100 and it took much longer to train. Now SVM took only 10 seconds, but after increasing, it took 9 minutes without completion, so I stopped it. SVM is bad already, NN takes 20 times longer so...)
    \end{itemize}
\end{itemize}


\end{document}